{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bbdc000",
   "metadata": {},
   "source": [
    "# Kaggle: YOLOv12n Training Pipeline (Complete)\n",
    "\n",
    "This notebook trains Ultralytics YOLOv12n on a YOLO-formatted dataset hosted in `/kaggle/input`. It includes:\n",
    "- Environment setup and dependency installation\n",
    "- Configuration and reproducibility\n",
    "- Accelerator detection (CPU/GPU/TPU) and AMP\n",
    "- Data sanity checks (counts, empty labels, class distribution)\n",
    "- Training with Ultralytics (mirrors your local hyperparameters)\n",
    "- Metrics visualization and inference preview\n",
    "- Artifact export to `/kaggle/working` for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Import and Install Dependencies\n",
    "# - Ensure ultralytics and common libs are available on Kaggle\n",
    "\n",
    "import sys, subprocess, os\n",
    "\n",
    "def pip_install(pkg):\n",
    "    print(f\"Installing {pkg}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "# Core\n",
    "pip_install(\"ultralytics>=8.1.0,<9\")\n",
    "pip_install(\"pandas>=2.0.0\")\n",
    "pip_install(\"numpy>=1.24.0\")\n",
    "pip_install(\"scikit-learn>=1.2.0\")\n",
    "pip_install(\"matplotlib>=3.7.0\")\n",
    "\n",
    "# Optional visualization tools\n",
    "try:\n",
    "    import cv2  # noqa\n",
    "except Exception:\n",
    "    pip_install(\"opencv-python\")\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, random, time, glob\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d47a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Configuration and Reproducibility\n",
    "\n",
    "class Cfg:\n",
    "    # Dataset slug uploaded to Kaggle Datasets (adjust to your dataset)\n",
    "    # Example: 'username/indian-urban-dataset-yolo'\n",
    "    dataset_slug = os.environ.get('DATASET_SLUG', 'username/indian-urban-dataset-yolo')\n",
    "\n",
    "    # Paths inside Kaggle\n",
    "    input_root = Path('/kaggle/input')\n",
    "    work_root = Path('/kaggle/working')\n",
    "    data_dirname = os.environ.get('DATA_DIRNAME', 'Indian_Urban_Dataset_yolo')  # optional subdir name if present\n",
    "\n",
    "    # Training\n",
    "    weights = os.environ.get('WEIGHTS', 'yolov12n.pt')\n",
    "    epochs = int(os.environ.get('EPOCHS', 80))\n",
    "    imgsz = int(os.environ.get('IMGSZ', 640))\n",
    "    batch = int(os.environ.get('BATCH', 16))\n",
    "    device = os.environ.get('DEVICE', '')  # ''=auto\n",
    "\n",
    "    # Reproducibility\n",
    "    seed = int(os.environ.get('SEED', 42))\n",
    "\n",
    "cfg = Cfg()\n",
    "\n",
    "# Seeding\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "\n",
    "# Resolve dataset mount dir\n",
    "DATASET_MOUNT = cfg.input_root / cfg.dataset_slug.split('/')[-1]\n",
    "assert DATASET_MOUNT.exists(), f\"Dataset mount not found at {DATASET_MOUNT}. Attach the dataset in Kaggle (Add Data).\"\n",
    "\n",
    "# If a zip exists inside the dataset mount, auto-extract the first zip\n",
    "import zipfile\n",
    "zip_candidates = list(DATASET_MOUNT.rglob('*.zip'))\n",
    "if zip_candidates:\n",
    "    extract_root = cfg.work_root / 'extracted_dataset'\n",
    "    extract_root.mkdir(parents=True, exist_ok=True)\n",
    "    print('Found zip:', zip_candidates[0])\n",
    "    with zipfile.ZipFile(zip_candidates[0], 'r') as zf:\n",
    "        zf.extractall(extract_root)\n",
    "    # If the zip contains a single top-level directory, use it; otherwise use extract_root\n",
    "    subdirs = [p for p in extract_root.iterdir() if p.is_dir()]\n",
    "    DATASET_DIR = subdirs[0] if len(subdirs) == 1 else extract_root\n",
    "else:\n",
    "    # Use the mount directly; if an expected subdir name exists, prefer it\n",
    "    candidate = DATASET_MOUNT / cfg.data_dirname\n",
    "    DATASET_DIR = candidate if candidate.exists() else DATASET_MOUNT\n",
    "\n",
    "print('DATASET_DIR =', DATASET_DIR)\n",
    "\n",
    "# Locate data.yaml by search if not at expected path\n",
    "import glob as _glob\n",
    "DATA_YAML = None\n",
    "candidates = list(DATASET_DIR.rglob('data.yaml'))\n",
    "if candidates:\n",
    "    DATA_YAML = candidates[0]\n",
    "else:\n",
    "    candidate = DATASET_DIR / 'data.yaml'\n",
    "    if candidate.exists():\n",
    "        DATA_YAML = candidate\n",
    "\n",
    "assert DATA_YAML is not None and DATA_YAML.exists(), f\"data.yaml not found anywhere under {DATASET_DIR}\"\n",
    "print('DATA_YAML   =', DATA_YAML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Detect Accelerator (CPU/GPU/TPU) and AMP\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    device_name = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "    print('Auto device =', device_name)\n",
    "except Exception as e:\n",
    "    print('Torch not available, defaulting to CPU', e)\n",
    "    device_name = 'cpu'\n",
    "\n",
    "amp_dtype = torch.float16 if device_name == 'cuda' else torch.bfloat16 if device_name == 'mps' else torch.float32\n",
    "print('AMP dtype =', amp_dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e730c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load Data from /kaggle/input (or extracted) and basic EDA\n",
    "\n",
    "import yaml\n",
    "with open(DATA_YAML, 'r') as f:\n",
    "    data_cfg = yaml.safe_load(f)\n",
    "\n",
    "print('data.yaml contents:')\n",
    "print(json.dumps(data_cfg, indent=2))\n",
    "\n",
    "# Derive absolute split paths\n",
    "# Support both (a) relative to a 'path' key and (b) direct split paths relative to DATASET_DIR\n",
    "base_path = DATASET_DIR\n",
    "if 'path' in data_cfg and data_cfg['path']:\n",
    "    # If data.yaml already has 'path', make sure it's consistent with our DATASET_DIR\n",
    "    # Prefer our resolved DATASET_DIR to avoid mismatches\n",
    "    data_cfg['path'] = str(base_path)\n",
    "\n",
    "split_paths = {}\n",
    "for k in ['train', 'val']:\n",
    "    raw = data_cfg[k]\n",
    "    p = base_path / raw if not str(raw).startswith('/') else Path(raw)\n",
    "    split_paths[k] = p\n",
    "    print(k, 'path =', p)\n",
    "    assert p.exists(), f\"Missing split path: {p}\"\n",
    "\n",
    "# Count images/labels and empty labels\n",
    "from collections import Counter\n",
    "\n",
    "def yolo_stats(labels_root: Path):\n",
    "    files = list(labels_root.rglob('*.txt'))\n",
    "    empty = 0\n",
    "    cls_count = Counter()\n",
    "    for p in files:\n",
    "        if p.stat().st_size == 0:\n",
    "            empty += 1\n",
    "            continue\n",
    "        for line in p.read_text().splitlines():\n",
    "            parts = line.strip().split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            cls_count[parts[0]] += 1\n",
    "    return len(files), empty, dict(sorted(cls_count.items(), key=lambda x: int(x[0])))\n",
    "\n",
    "# Infer labels directory from image directory parent structure\n",
    "for split in ['train', 'val']:\n",
    "    img_dir = split_paths[split]\n",
    "    # Typically: datasets/images/train and datasets/labels/train\n",
    "    labels_dir = img_dir.parents[1] / 'labels' / split if len(img_dir.parents) >= 2 else img_dir.parent.parent / 'labels' / split\n",
    "    num_labels, empty_labels, dist = yolo_stats(labels_dir)\n",
    "    num_images = len(list(img_dir.rglob('*.jpg'))) + len(list(img_dir.rglob('*.png')))\n",
    "    print(f\"Split={split}: images={num_images}, label_files={num_labels}, empty_labels={empty_labels}\")\n",
    "    print('class distribution:', dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29adbedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch data.yaml for Kaggle runtime (write to /kaggle/working)\n",
    "\n",
    "import yaml\n",
    "WORK_DATA_YAML = cfg.work_root / 'data_kaggle.yaml'\n",
    "\n",
    "with open(DATA_YAML, 'r') as f:\n",
    "    cfg_yaml = yaml.safe_load(f)\n",
    "\n",
    "# Ensure 'path' points to the dataset directory under /kaggle/input\n",
    "cfg_yaml['path'] = str(DATASET_DIR / cfg.data_dirname)\n",
    "\n",
    "with open(WORK_DATA_YAML, 'w') as f:\n",
    "    yaml.safe_dump(cfg_yaml, f)\n",
    "\n",
    "print('Wrote patched data.yaml to:', WORK_DATA_YAML)\n",
    "print('Using path:', cfg_yaml['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5072bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Training with Ultralytics YOLO\n",
    "\n",
    "# Mirror your local hyperparameters\n",
    "print('Starting training...')\n",
    "model = YOLO(cfg.weights)\n",
    "train_result = model.train(\n",
    "    data=str(WORK_DATA_YAML),\n",
    "    epochs=cfg.epochs,\n",
    "    imgsz=cfg.imgsz,\n",
    "    batch=cfg.batch,\n",
    "    device=cfg.device,  # ''=auto; Kaggle will pick GPU if available\n",
    "    project=str(cfg.work_root),\n",
    "    name='yolov12n_indian_urban_kaggle',\n",
    "    exist_ok=True,\n",
    ")\n",
    "print(train_result)\n",
    "\n",
    "# Locate artifacts\n",
    "ART_DIR = cfg.work_root / 'yolov12n_indian_urban_kaggle'\n",
    "print('Artifacts at:', ART_DIR)\n",
    "assert ART_DIR.exists(), 'Training artifact directory missing'\n",
    "WEIGHTS_BEST = ART_DIR / 'weights' / 'best.pt'\n",
    "WEIGHTS_LAST = ART_DIR / 'weights' / 'last.pt'\n",
    "print('Best weights:', WEIGHTS_BEST)\n",
    "print('Last weights:', WEIGHTS_LAST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45302217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Metrics visualization (results.csv, results.png)\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_csv = ART_DIR / 'results.csv'\n",
    "results_png = ART_DIR / 'results.png'\n",
    "\n",
    "if results_csv.exists():\n",
    "    df = pd.read_csv(results_csv)\n",
    "    display(df.tail(3))\n",
    "    print('Final row:')\n",
    "    display(df.iloc[[-1]])\n",
    "else:\n",
    "    print('results.csv not found at', results_csv)\n",
    "\n",
    "if results_png.exists():\n",
    "    from IPython.display import Image, display as disp\n",
    "    disp(Image(filename=str(results_png)))\n",
    "else:\n",
    "    print('results.png not found at', results_png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5031e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Inference preview on a few validation images\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "val_dir = DATASET_DIR / data_cfg['val']\n",
    "assert val_dir.exists()\n",
    "\n",
    "subset = sorted(glob.glob(str(val_dir / '*.jpg')))[:8]\n",
    "if not subset:\n",
    "    subset = sorted(glob.glob(str(val_dir / '*.png')))[:8]\n",
    "\n",
    "print('Preview images:', len(subset))\n",
    "\n",
    "model_inf = YOLO(str(WEIGHTS_BEST if WEIGHTS_BEST.exists() else WEIGHTS_LAST))\n",
    "# Save predictions under the training artifacts folder for easy browsing\n",
    "res = model_inf.predict(source=subset, imgsz=cfg.imgsz, conf=0.25,\n",
    "                        project=str(ART_DIR), name='predict', save=True)\n",
    "\n",
    "# Show the result images saved by Ultralytics in ART_DIR/predict\n",
    "pred_dir = ART_DIR / 'predict'\n",
    "print('Predictions directory:', pred_dir)\n",
    "\n",
    "if pred_dir.exists():\n",
    "    imgs = sorted(glob.glob(str(pred_dir / '*.jpg')))\n",
    "    if not imgs:\n",
    "        imgs = sorted(glob.glob(str(pred_dir / '*.png')))\n",
    "    for p in imgs[:6]:\n",
    "        display(Image(filename=p))\n",
    "else:\n",
    "    print('No prediction images found (Ultralytics may have saved elsewhere)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f35285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch data.yaml for Kaggle runtime (write to /kaggle/working)\n",
    "\n",
    "import yaml\n",
    "WORK_DATA_YAML = cfg.work_root / 'data_kaggle.yaml'\n",
    "\n",
    "with open(DATA_YAML, 'r') as f:\n",
    "    cfg_yaml = yaml.safe_load(f)\n",
    "\n",
    "# Ensure 'path' points to our resolved DATASET_DIR\n",
    "cfg_yaml['path'] = str(DATASET_DIR)\n",
    "\n",
    "with open(WORK_DATA_YAML, 'w') as f:\n",
    "    yaml.safe_dump(cfg_yaml, f)\n",
    "\n",
    "print('Wrote patched data.yaml to:', WORK_DATA_YAML)\n",
    "print('Using path:', cfg_yaml['path'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7671d9a",
   "metadata": {},
   "source": [
    "# Quick Start (Kaggle)\n",
    "\n",
    "1) Add your dataset to Kaggle Datasets. Its slug will look like `username/indian-urban-dataset-yolo`.\n",
    "2) Open a new Kaggle Notebook (GPU if available), add the dataset via the sidebar.\n",
    "3) In the config cell, set env vars if needed:\n",
    "   - `DATASET_SLUG` (e.g., `username/indian-urban-dataset-yolo`)\n",
    "   - `WEIGHTS` (default `yolov12n.pt`), `EPOCHS`, `IMGSZ`, `BATCH`, `DEVICE`\n",
    "4) Run all cells top-to-bottom.\n",
    "5) Outputs:\n",
    "   - Artifacts: `/kaggle/working/yolov12n_indian_urban_kaggle` (weights, results.csv/png)\n",
    "   - Zipped bundle: `/kaggle/working/yolov12n_artifacts.zip`\n",
    "   - Convenience copies: `/kaggle/working/best.pt`, `/kaggle/working/last.pt`\n",
    "6) Download the zip and `best.pt` from the right-hand “Output” panel.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
